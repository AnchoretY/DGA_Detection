{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 冠军解决方案复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"../DataCon2020/dga/dns_2_question_a1805e67f3a33814e3eb5d5ce609996299b3835b/domains_1.txt\",names=['domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "域名总数:14110\n"
     ]
    }
   ],
   "source": [
    "print(\"域名总数:{}\".format(df1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取域名记录个数，过滤大于3的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import whois\n",
    "import  dns.resolver\n",
    "\n",
    "\n",
    "def get_DNS_Record_Nums(domain):\n",
    "    \"\"\"\n",
    "        获取DGA域名的条数\n",
    "        Parameter:\n",
    "        -----------------------\n",
    "            domain: 需要进行查询记录数的域名\n",
    "        Return:\n",
    "            域名对应的记录数量\n",
    "    \"\"\"\n",
    "    record_nums = 0\n",
    "    try:\n",
    "        result = dns.resolver.resolve(domain)\n",
    "        answer = result.response.answer\n",
    "        for i,data in enumerate(answer):\n",
    "            record_nums += len(data)\n",
    "        return record_nums\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 16s, sys: 2.11 s, total: 1min 18s\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel,delayed\n",
    "\n",
    "def tmp_func(df1):\n",
    "    tqdm.pandas(ncols=50)\n",
    "    df1[\"dns_record_nums\"] = df1.progress_apply(lambda x:get_DNS_Record_Nums(x.domain),axis=1)\n",
    "    return df1\n",
    "                  \n",
    "def apply_parallel(df_grouped,func):\n",
    "    results = Parallel(n_jobs=30)(delayed(func)(group) for name,group in df_grouped)\n",
    "    return pd.concat(results)\n",
    "    \n",
    "df_grouped = df1.groupby(df1.index)\n",
    "%time df1 = apply_parallel(df_grouped,tmp_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤DNS记录数大于3的域名后剩余域名数量:12955\n"
     ]
    }
   ],
   "source": [
    "df1 = df1[df1['dns_record_nums']<3]\n",
    "print(\"过滤DNS记录数大于3的域名后剩余域名数量:{}\".format(df1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取域名对应的子域名个数，过滤大于3的域名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import sys\n",
    "\n",
    "def get_subdomain(url):\n",
    "    \"\"\"\n",
    "        使用ip138网获取域名的子域名   \n",
    "        Parameters:\n",
    "        -----------------------------\n",
    "            url: 要进行查询的url\n",
    "        Return:\n",
    "        -----------------------------\n",
    "            subdomain_list:子域名列表\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36',\n",
    "        'Cookie': 'PHPSESSID=8d3k4g8sub7s34mn73c8r12aq7; Hm_lvt_d39191a0b09bb1eb023933edaa468cd5=1599205303; PHPSESSID=skmtbkigevkdbvaj2jq311gtne; Hm_lpvt_d39191a0b09bb1eb023933edaa468cd5=1599206267'\n",
    "    }\n",
    "    \n",
    "    target = url\n",
    "    api = \"http://site.ip138.com/%s/domain.htm\" % target\n",
    "    \n",
    "    try:\n",
    "        req = requests.get(api,headers=headers,timeout=10)\n",
    "        html = req.text\n",
    "    except Exceptions as e:\n",
    "        html = ''\n",
    "        print(e)\n",
    "        \n",
    "    # 使用正则进行匹配，匹配页面上返回的子域名\n",
    "    re_subdomains = re.findall(r\"\\\"_blank\\\">(.*?)</a></p>\",html)\n",
    "    returnlist = re_subdomains\n",
    "    return returnlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 1.56 s, total: 1min 29s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "def tmp_func(df1):\n",
    "    tqdm.pandas(ncols=50)\n",
    "    df1[\"subdomain\"] = df1.progress_apply(lambda x:len(get_subdomain(x.domain)),axis=1)\n",
    "    return df1\n",
    "                  \n",
    "def apply_parallel(df_grouped,func):\n",
    "    results = Parallel(n_jobs=30)(delayed(func)(group) for name,group in df_grouped)\n",
    "    return pd.concat(results)\n",
    "    \n",
    "df_grouped = df1.groupby(df1.index)\n",
    "%time df1 = apply_parallel(df_grouped,tmp_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤子域名大于3的域名后剩余可疑域名个数11173\n"
     ]
    }
   ],
   "source": [
    "df1 = df1[df1['subdomain']<3]\n",
    "print(\"过滤子域名大于3的域名后剩余可疑域名个数{}\".format(df1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过滤alexa 1m中的域名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alexa = pd.read_csv(\"./../../../data/alexa-top-1m.csv\",names=['rank','domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[~df1['domain'].isin(df_alexa['domain'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤alexa域名1m后剩余域名数量为9973\n"
     ]
    }
   ],
   "source": [
    "print(\"过滤alexa域名1m后剩余域名数量为{}\".format(df1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用英文进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tldextract\n",
    "\n",
    "def get_contain_english_words(input_str):\n",
    "    \"\"\"\n",
    "        获取域名中的英文单词，使用SCOWL提供的单词表进行匹配，其中包含了8万多个最常见的英文单词，这里只匹配长度大于2的英文单词\n",
    "        Parameters:\n",
    "        ----------------------\n",
    "            input_str: 输入字符串，这里是指域名\n",
    "        Return:\n",
    "        -----------------------\n",
    "            word_found: 域名中匹配到的单词列表\n",
    "    \"\"\"\n",
    "    dictionary = set(open('SCOWL-wordlist.txt','r').read().split())\n",
    "    max_len = max(map(len, dictionary))\n",
    "    \n",
    "    input_str = input_str.lower().rstrip()\n",
    "    \n",
    "    # 域名中去掉顶级域名、去掉空格\n",
    "    extracted = tldextract.extract(input_str)\n",
    "    justfound = extracted.domain.replace(\" \",\"\")\n",
    "    \n",
    "    # 遍历整个域名，按照英文字典中最大域名长度划分chunk，然后从1个字符到chunk长度遍历与英文字典进行匹配\n",
    "    words_found = set() #set of words found, starts empty\n",
    "    for i in range(len(justfound)): \n",
    "        chunk = input_str[i:i+max_len+1] \n",
    "        for j in range(1,len(chunk)+1): \n",
    "            word = chunk[:j] #subchunk\n",
    "            if word in dictionary: \n",
    "                if len(word) > 2: words_found.add(word)\n",
    "                    \n",
    "    words_found = sorted(words_found)\n",
    "    number_of_words = len(words_found)\n",
    "    \n",
    "\n",
    "    return words_found\n",
    "\n",
    "def get_max_english_words_len(domain):\n",
    "    \"\"\"\n",
    "        获取域名中能匹配到的最长英文单词长度\n",
    "        Parameters:\n",
    "        -----------------------------\n",
    "            domain: 要进行匹配的字符串\n",
    "        Return:\n",
    "        -----------------------------\n",
    "            max_len: 匹配到最长的英文单词长度\n",
    "    \"\"\"\n",
    "    words_list = get_contain_english_words(domain)\n",
    "    if words_list==[]:\n",
    "        return 0\n",
    "    return max([len(i) for i in words_list])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "from joblib import Parallel,delayed\n",
    "\n",
    "\n",
    "def tmp(df):\n",
    "    df['max_english_word_len'] = df.apply(lambda x:get_max_english_words_len(x.domain),axis=1)\n",
    "    return df\n",
    "\n",
    "def apply_Parallel(df_grouped,func):\n",
    "    results = Parallel(n_jobs=30)(delayed(func)(group) for name,group in df_grouped)\n",
    "    return pd.concat(results)\n",
    "\n",
    "\n",
    "df1_grouped = df1.groupby(df1.index)\n",
    "df1 = apply_Parallel(df1_grouped,tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除包含英文单词的域名后剩余域名个数:1795\n"
     ]
    }
   ],
   "source": [
    "df1 = df1[df1['max_english_word_len']<4]\n",
    "print(\"去除包含英文单词的域名后剩余域名个数:{}\".format(df1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用拼音进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_pinyin_dicionary(dictionary_file=\"pinyin.txt\"): \n",
    "    \"\"\"\n",
    "        从原始常见拼音文件中得到特定格式的字典，处理主要包含两部分：\n",
    "            1.\"hao/fei\" -> \"hao\"\n",
    "                           \"fei\"\n",
    "            2.去除长度小于2的拼音，因为即使成功匹配也可能存在很大偶然性\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    dictionary = set(open(dictionary_file,'r').read().split())\n",
    "    \n",
    "    # 遍历\n",
    "    pinyin_dictionary = set()\n",
    "    for i in dictionary:\n",
    "        if '/' in i:\n",
    "            l = i.split(\"/\")\n",
    "            for j in l:\n",
    "                if len(j)>2:\n",
    "                    pinyin_dictionary.add(j)\n",
    "        else:\n",
    "            if len(i)>2:\n",
    "                pinyin_dictionary.add(i)\n",
    "                \n",
    "    return pinyin_dictionary\n",
    "    \n",
    "#     # 存储为csv格式            \n",
    "#     pd.DataFrame(pinyin_dictionary).to_csv(\"pinyin.csv\",index=False,header=False)\n",
    "    \n",
    "    \n",
    "def get_contain_pinyin_words(input_str):\n",
    "    \"\"\"\n",
    "        获取域名全部能够匹配到的拼音\n",
    "        Parameters:\n",
    "        -----------------------------\n",
    "            domain: 要进行匹配的字符串\n",
    "        Return:\n",
    "        -----------------------------\n",
    "            words_found: 匹配到的拼音列表\n",
    "    \"\"\"\n",
    "    dictionary = _get_pinyin_dicionary(dictionary_file=\"pinyin.txt\")\n",
    "    max_len = max(map(len, dictionary))\n",
    "    \n",
    "    input_str = input_str.lower().rstrip()\n",
    "    \n",
    "    # 域名中去掉顶级域名、去掉空格\n",
    "    extracted = tldextract.extract(input_str)\n",
    "    justfound = extracted.domain.replace(\" \",\"\")\n",
    "    \n",
    "    # 遍历整个域名，按照英文字典中最大域名长度划分chunk，然后从1个字符到chunk长度遍历与英文字典进行匹配\n",
    "    words_found = set() #set of words found, starts empty\n",
    "    for i in range(len(justfound)): \n",
    "        chunk = input_str[i:i+max_len+1] \n",
    "        for j in range(1,len(chunk)+1): \n",
    "            word = chunk[:j] #subchunk\n",
    "            if word in dictionary: \n",
    "                if len(word) > 2: words_found.add(word)\n",
    "                    \n",
    "    words_found = sorted(words_found)\n",
    "    number_of_words = len(words_found)\n",
    "    \n",
    "\n",
    "    return words_found\n",
    "\n",
    "def get_pinyin_nums(domain):\n",
    "    \"\"\"\n",
    "        获取能够匹配到的拼音数量\n",
    "        Parameters:\n",
    "        -----------------------------\n",
    "            domain: 域名字符串\n",
    "        Return:\n",
    "        -----------------------------\n",
    "            pinyin_nums: 域名中能够匹配到的拼音数量\n",
    "        \n",
    "    \"\"\"\n",
    "    words_list = get_contain_pinyin_words(domain)\n",
    "    return len(words_list)\n",
    "\n",
    "\n",
    "def tmp(df):\n",
    "    df['pinyin_nums'] = df.apply(lambda x:get_pinyin_nums(x.domain),axis=1)\n",
    "    return df\n",
    "\n",
    "def apply_Parallel(df_grouped,func):\n",
    "    results = Parallel(n_jobs=30)(delayed(func)(group) for name,group in df_grouped)\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_grouped = df1.groupby(df1.index)\n",
    "df1 = apply_Parallel(df1_grouped,tmp)\n",
    "\n",
    "df1 = df1[df1['pinyin_nums']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用拼音进行过滤后剩余可以域名数量:238\n"
     ]
    }
   ],
   "source": [
    "print(\"使用拼音进行过滤后剩余可以域名数量:{}\".format(df1.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据熵值对域名进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_entropy(text):\n",
    "    h = 0.0\n",
    "    sum = 0\n",
    "    letter = [0] * 26\n",
    "    text = text.lower()\n",
    "    for i in range(len(text)):\n",
    "        if text[i].isalpha():\n",
    "            letter[ord(text[i]) - ord('a')] += 1\n",
    "            sum += 1\n",
    "    for i in range(26):\n",
    "        p = 1.0 * letter[i] / sum\n",
    "        if p > 0:\n",
    "            h += -(p * math.log(p, 2))\n",
    "    return \"%.4f\"%(h)\n",
    "\n",
    "\n",
    "df1['entorpy'] = df1.apply(lambda x:cal_entropy(tldextract.extract(x.domain).domain),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sort_values(\"entorpy\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>dns_record_nums</th>\n",
       "      <th>subdomain</th>\n",
       "      <th>max_english_word_len</th>\n",
       "      <th>pin_nums</th>\n",
       "      <th>pinyin_nums</th>\n",
       "      <th>entorpy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11571</th>\n",
       "      <td>tvznmabcdefg.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7810</th>\n",
       "      <td>mycupofteahk.com</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12198</th>\n",
       "      <td>wlcbsjnqrmfy.com</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>akwsryilodjt.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054</th>\n",
       "      <td>gjdferqmiiuz.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13389</th>\n",
       "      <td>ytslsqshflzx.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12291</th>\n",
       "      <td>wtjcqntwdtgs.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12825</th>\n",
       "      <td>xogkpytfgyzy.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14053</th>\n",
       "      <td>znfwfqoeogfi.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9103</th>\n",
       "      <td>rajivramnath.com</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 domain  dns_record_nums  subdomain  max_english_word_len  \\\n",
       "11571  tvznmabcdefg.com                0          1                     3   \n",
       "7810   mycupofteahk.com                1          2                     3   \n",
       "12198  wlcbsjnqrmfy.com                1          1                     3   \n",
       "177    akwsryilodjt.com                0          1                     0   \n",
       "4054   gjdferqmiiuz.com                0          1                     3   \n",
       "...                 ...              ...        ...                   ...   \n",
       "13389  ytslsqshflzx.com                0          1                     0   \n",
       "12291  wtjcqntwdtgs.com                0          1                     0   \n",
       "12825  xogkpytfgyzy.com                0          1                     0   \n",
       "14053  znfwfqoeogfi.com                0          1                     0   \n",
       "9103   rajivramnath.com                1          1                     3   \n",
       "\n",
       "       pin_nums  pinyin_nums entorpy  \n",
       "11571         0            0  3.5850  \n",
       "7810          0            0  3.5850  \n",
       "12198         0            0  3.5850  \n",
       "177           0            0  3.5850  \n",
       "4054          0            0  3.4183  \n",
       "...         ...          ...     ...  \n",
       "13389         0            0  3.0221  \n",
       "12291         0            0  3.0221  \n",
       "12825         0            0  3.0221  \n",
       "14053         0            0  3.0221  \n",
       "9103          0            0  3.0221  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['domain'].head(100).to_csv(\"dga2_1_result.csv\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
